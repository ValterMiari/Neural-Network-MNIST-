{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network that classifies numbers from images\n",
    "#### Trained on the MNIST data set with Python, using the Keras framework. Utalizes convolutional layers to achieve better results, as well as auto-encoders for denoising. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters data-loading and formatting\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n",
    "\n",
    "# reshape x_train and x_test to 4 dimensional matrix\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation of the data pre-processing\n",
    "The MNIST function load_data() returns a tuple of numpy arrays, (x_train, y_train), (x_test,y_test). The numpy arrays *x_train* and *x_test* contains numerical representations of the grayscale images of the 10 digits, in the form of a pixel values between 0 and 255. The pixel values represents the brightness level of the pixel, 0 is black and  255 is white. Each element in the numpy arrays represents a pixel and holds a value representing its brightness. \n",
    "\n",
    "The pixel values are first converted to floats, so that no raounding error occurs, then the pixels are scaled by 255, such that its value is between 0 and 1.  \n",
    "The *lbl_train* and *lbl_test* are vectors of length #images, that represent the digits that the images are displaying. These vectors are converted to binary matrices of #imagesx#digits <=> 6000x10 with the function *to_categorical()*, where each row represent a digit as a 1 at its corresponding index and all others as zeroes, e.g. $l = [1, 8], \\space to\\_categoriacal(l) = [[0,1,0,0,0,0,0,0,0,0], [0,0,0,0,0,0,0,0,1,0]]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_data gives data of type uint-8, which are integers\n",
    "# change to float so as to not have rounding errors\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# changing grey-scale from between 0 & 255 to between 0 & 1\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# .to_categorical() returns \"A binary matrix representation of the input. The classes axis is placed last.\"\n",
    "# converts the expected number to from a decimal to a set of 10 binary numbers\n",
    "# the binary number at the \"correct position\" is set to 1\n",
    "# i.e. if predicted number is 5 at index 3, then the binary number at index 5 is set to 1 on the third row.\n",
    "y_train = keras.utils.to_categorical(lbl_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(lbl_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The number of neurons in each layer**\n",
    "\n",
    "1) input layer: 784\n",
    "2) hidden layer: 64\n",
    "3) hidden layer: 64\n",
    "4) output layer: 10 \n",
    "\n",
    "There are a total of 4 layers in this network, one input with 784 neurons (from the 28 x 28 grid of pixels), 2 hidden layers with 64 neurons each with rectified linear activation functions and finally the output layer with 10 neurons for the 10 digits with the softmax activation layer.\n",
    "\n",
    "**Activation functions and why they are appropriate for this application**\n",
    "\n",
    "The Rectified Linear Unit function or ReLu, is the activation function used in the two hidden layers. The ReLu activation function is constructed as: $f(x) = max(0, x)$. It is appropriate for its minimal computational demand, which speeds up the training process compared to other activation functions such, as *sigmoid* or *tanh*. Also, ReLu function doesn't have an asymptotic lower or higher bound, so errors can more easily be accounted for in the first hidden layer, as well as the sparcity of the output, i.e. most values are outputed as zeroes. (not finished)\n",
    "\n",
    "**Total number of parameters for the network**\n",
    "\n",
    "Overall, there are 55,050 parameters that the model has to fine tune (train) in the neurons in order to minimize the cost function and get a good model.\n",
    "\n",
    "**Why the input and output layers have the dimensions they have**\n",
    "\n",
    "The input layer have 784 dimensions due to the amount of pixels in the images 28x28 = 784. The output layer has 10 dimensions since there are 10 digits and therefore 10 possible outcomes and the purpose of the model is to predict a digit given an image.\n",
    "\n",
    "**The loss function used to train the network and how to interpret it**\n",
    "\n",
    "The loss function used is Cross-entropy (or log) loss. It calculates the performance of the classification model that outputs a probability value (i.e. between 0 and 1). The higher the difference between the given probability value and the true value, the higher the loss value. This function can deal with outputs or classes of large numbers which makes it a suitable choice for our model as we have 10 clases and output a value between 0 and 1.\n",
    "\n",
    "the functional form is:\n",
    "\n",
    "$L = -\\sum_{o=1}^N{\\sum_{c=1}^M{y_{o,c}\\log{p_{o,c}}}}$\n",
    "\n",
    "where:\n",
    "* L is the loss value returned by this function\n",
    "* M is the number of classess/predictions (represented as *c*) i.e. the 10 digits we are trying to interpret\n",
    "* N is the total number of observations (represented as *o*) i.e. it is the number of test data\n",
    "* log is the natural log\n",
    "* y is the binary indicator if the class label *c* is the right label/class for the predicted label or observation *o*\n",
    "* p is the predicted probability that the observation *o* is of class *c*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## Define model ##\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSequential\u001b[49m()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# converts the 28 x 28 matrix to a single row vector of inputs that is 784 (28 times 28) long\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "## Define model ##\n",
    "model = Sequential()\n",
    "# converts the 28 x 28 matrix to a single row vector of inputs that is 784 (28 times 28) long\n",
    "model.add(Flatten())\n",
    "# 2 hidden layers, rectified linear activation function (i.e. linear at positive axis)\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "# output layer, softmax activation function\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# total of 4 layers\n",
    "\n",
    "# keras.losses.categorical_crossentropy is loss function\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "               optimizer=keras.optimizers.SGD(lr = 0.1),\n",
    "        metrics=['accuracy'],)\n",
    "\n",
    "fit_info = model.fit(x_train, y_train,\n",
    "           batch_size=batch_size,\n",
    "           epochs=epochs,\n",
    "           verbose=1,\n",
    "           validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots of the trained model. For trainnig and test/validation accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_acc(accuracy, val_accuracy, title=\"\"):\n",
    "    plt.plot(accuracy)\n",
    "    plt.plot(val_accuracy)\n",
    "    plot_title = title if title else 'Handwritten digit classification'\n",
    "    plt.title(plot_title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Training set', 'Test set'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def graph_acc_bar(accuracies, standard_deviations, regularizations):\n",
    "    # Build the plot\n",
    "    x_pos = np.arange(len(regularizations))\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    ax.bar(x_pos, accuracies, yerr=standard_deviations, align='center', alpha=0.5, ecolor='black', capsize=10)\n",
    "    ax.set_ylabel('mean accuracy reached by models')\n",
    "    ax.set_xlabel('regularization factors')\n",
    "    ax.set_xticks(x_pos)\n",
    "    ax.set_xticklabels(regularizations)\n",
    "    plt.ylim([0.97, 1])\n",
    "    ax.set_title('effect of regularization factor on model accuracy')\n",
    "    ax.yaxis.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "graph_acc(fit_info.history['accuracy'], fit_info.history[\"val_accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Updated model, using a three-layered neural network, where the hidden-layers have 500 and 300 hidden units respectively. Trained for 40 epochs. Geoff Hinton claimed that this network could reach a validation accuracy of 0.9847 (http://yann.lecun.com/exdb/mnist/) using weight decay (L2 regularization of weights (kernels): https://keras.io/api/layers/regularizers/))\n",
    "#### The code below is an implementation of weight decay on hidden units, where five regularization factors from 0.000001 to 0.001 have been selected. Three replicated networks have been trained for each gegularization factor. The final validation accuracy is plotted with standard deviation (computed from the replicates) as a function of the regularization factor. \n",
    "\n",
    "The range of regularization factors used yielded accuracies that were quite close to the maximum that Hinton achieved.\n",
    "The differences from my result to Hintons result can be attributed to the fact that:\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>The regularization factors are quite small compared to the large number of datapoints used. As the regularization factor is proportional to the chosen factor and inversely proportional to the amount of data, having a large dataset can lead to minimized effect from the weight regularization.</li>\n",
    "  <li>It is likely that the model is finding local minima rather than the desired global minimum which would have given the highest accuracy hence the relatively large variation in the model validation accuracies.</li>\n",
    "  <li>It is possible that using only 40 epochs to train a considerably large network is not enough to reach the accuracy. In other words, using only 40 epochs may be ending the training period shorter than what is needed to optimize the model using the training data.</li>\n",
    "</ol> \n",
    "\n",
    "The exact results can be seen in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating help functions to create 15 models (3 repeats for 5 different regularization factors) ##\n",
    "epochs_hinton = 40\n",
    "def get_hinton_model(l2_value):\n",
    "    model_hinton = Sequential()\n",
    "    model_hinton.add(Flatten())\n",
    "    model_hinton.add(Dense(500, activation = 'relu', kernel_regularizer = keras.regularizers.l2(l2_value)))\n",
    "    model_hinton.add(Dense(300, activation = 'relu', kernel_regularizer = keras.regularizers.l2(l2_value)))\n",
    "    model_hinton.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model_hinton.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                optimizer=keras.optimizers.SGD(lr = 0.1),\n",
    "            metrics=['accuracy'],)\n",
    "    return model_hinton\n",
    "\n",
    "def fit_and_eval(model, x_train, y_train, epochs, x_validation, y_validation, show_prog=1):\n",
    "    fit_info = model.fit(x_train, y_train,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            verbose=show_prog, # set to 0 to avoid updates on each epoch to reduce clutter, otherwise set to 1\n",
    "            validation_data=(x_validation, y_validation))\n",
    "    score = model.evaluate(x_validation, y_validation, verbose=0)\n",
    "    print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))\n",
    "    return fit_info, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes at most 1.5 hours to train and tune model in a separate notebook\n",
    "# seems here it would take approx 4.25 hours to train\n",
    "\n",
    "optimum_models = {}\n",
    "regularization_factors = np.linspace(0.00001, 0.0006, 5)\n",
    "mean = np.zeros(len(regularization_factors))\n",
    "std = np.zeros(len(regularization_factors))\n",
    "for factor in regularization_factors:\n",
    "    print(f\"training with a weight decay factor of {factor}\")\n",
    "    repetition_models = {}\n",
    "    model_acc = np.zeros(3)\n",
    "    for i in range(0,3):\n",
    "        print(f\"iteration number {i}\")\n",
    "        current_model = get_hinton_model(factor)\n",
    "        curr_fit, curr_score = fit_and_eval(current_model, x_train, y_train, epochs_hinton, x_test, y_test, 0)\n",
    "        model_acc[i] = curr_score[1]\n",
    "    index = np.where(regularization_factors == factor)[0][0]\n",
    "    std[index] = np.std(model_acc) # standard deviation in repeated models accuracy\n",
    "    mean[index] = np.mean(model_acc) # mean accuacy of repeated models\n",
    "    print(f'Model achieved mean accuracy of {mean[index]} plus minus {std[index]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid running the block above everytime the variable is needed, the latest iteration is saved in the variables below\n",
    "mean = [0.9820999900499979, 0.9820999900499979, 0.9821333289146423, 0.9823333223660787, 0.9812333385149637]\n",
    "std = [0.00045460719637993325, 0.0005099125311197117, 0.0004988960965998952, 0.0005436563716422689, 0.0019601390285467588]\n",
    "regularization_factors = np.linspace(0.00001, 0.0006, 5)\n",
    "graph_acc_bar(mean, std, regularization_factors)\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### According to the MNIST database it should be possible to reach 99% accuaracy on the validation data. Below is an implementation of a model using convolutional layers, that has an accuracy greater than 99%. As well as an explanation of the model and the choices made in the process of creating it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal regulization_factor for the first dense layer\n",
    "optimum_l2_value = 0.0001\n",
    "\n",
    "# Creating the model for the CNN\n",
    "conv_model = Sequential([\n",
    "    Conv2D(32, (3, 3), padding='same', activation = 'relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), padding='same', activation = 'relu'),\n",
    "    Conv2D(64, (3, 3), padding='same', activation = 'relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(100, activation= 'relu', kernel_regularizer = keras.regularizers.L2(optimum_l2_value)),\n",
    "    Dense(num_classes, activation='softmax'),\n",
    "    # A total of five layers and two max pooling sequences. Three convolutional layers and two dense layers.\n",
    "    # The ReLu activation function is used for all layers except for the output layer that uses softmax.\n",
    "])\n",
    "\n",
    "# Compiling the model, keras.losses.categorical_crossentropy used as the loss function\n",
    "conv_model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "    optimizer=keras.optimizers.SGD(lr = 0.1), \n",
    "    metrics=['accuracy'],)\n",
    "\n",
    "#conv_model.save(\"Best_performing_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with the digit-data and evaluate the CNN-model\n",
    "conv_fit, conv_score = fit_and_eval(conv_model, x_train, y_train, 10, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Highest achieved accuracy on the validation data:', max(conv_fit.history['val_accuracy']))\n",
    "graph_acc(conv_fit.history['accuracy'], conv_fit.history['val_accuracy'],\n",
    "    title = 'Digit classification from the CNN-model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the process of trying to create a CNN-model that achived 99%+ accuracy on the validation data, I came up with several different models and approaches. All models I tried included at least one convolutional layer and at most three, one to two max-pooling sequences and different amount of neurons in the first dense layer, ranging from 64-784, as well as an additional dense layer, apart from the output layer, with the same range of neurons. \n",
    "\n",
    "The model wI decided on is the one displayed above, which is the best performing model I managed to create. It predicts the validation data with just above 99% accuracy, more specifically $99.05999898910522\\%$ accuracy at best.\n",
    "The CNN-model consists of five layers and two pooling sequences. The feature learning, i.e. the layers and sequences that detects features in the images, consists of three convolutional layers and two max-pooling sequences. The classification, i.e. the layer(s) that classifies the data consists of two dense layers, where one of them is the output layer. \n",
    "The first layer is a convolutional layer with 32 filters and filter grid of size 3x3. This layer is followed up with a max-pooling sequence with the size of 2x2. Max-pooling scans the feature maps that the convolutional layer outputs one grid-size at the time, and chooses the maximum value from the feature map covered in the grid. This reduces the dimensions of the feature map which in turn reduces the amount of parameters, which lowers the computational cost, it also helps preventing overfitting. The two predeceeding layers are convolutional layers with 64 filters each. An additional max-pooling sequence is applied afterwards with a 2x2 grid, before flattening the data and classifying it, by means of a dense layer with 100 neurons. \n",
    "\n",
    "Dropout was not used in the model. The reasoning behind this choice is that dropouts main purpose is to prevent overfitting of a model, as well as the accuracy of the model being above 99% without using it. For this particular model, overfitting isn't necessarily a problem, since we don't intend to use this model for anything other than digit classification/prediction. The two max-pooling sequences helps in preventing overfitting, since they are used, I believe there is no need for dropout. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use convolutional layers over fully connected onces for the particular application.**\n",
    "\n",
    "Convolutional layers allows for the neural network to detect certain patterns in the data, like edges, circles or more complex ones like eyes or accessories, by convolving kernels across the input data. When multiple convolutional layers are applied to a neural network, the patterns are combined, enableing detection of more complex patterns or features in the data. \n",
    "\n",
    "Parameter sharing, which is achived by using one kernel throughout the input image, is a desirable component of convolutional layers, which isn't present in fully connected ones. It leads to fewer learned parameters and increases the execution speed with additional help from max pooling, as mentioned above. Too many parameters makes a model prone to overfitting as well, which, when comparing the two, indicates that fully connected layers are more prone to overfitting that convolutional ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auto-Encoder for denoising\n",
    "\n",
    "In order to work with the model and the noise function, the data is reshaped into a single row vector that is 784 dimensional (28x28) where the grid is flattened, such that the pixels are on a single line. Then the noise function can be applied to get a noisy dataset in order to compare the efficiency of the model used. The model uses 2 layers for compressing and encoding the input and another 2 (output included) to decode said compression. The compression factor is $\\frac{784}{96}$ which is approximately a factor of 8. The model should be able to reconstruct the input after compression with minimal losses. One can see how the model does this compression and decompression through the decreased number of neurons halfway through (in the encoding layers) where compression happens in 2 steps: first by feeding 784 values to 128 neurons and then feeding the output of those neurons to 96 neurons, effectively reducing the number of datapoints needed to represent a unique digit in a 28x28 pixel grid. The opposite is done in the decompression stage (decoding layers) where the 96 neurons feed to 128 which in turn feed into 784 outputs that can be used to \"create\" the reconstructed digit in a 28x28 pixel grid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def salt_and_pepper(input, noise_level=0.5):\n",
    "    \"\"\"\n",
    "    This applies salt and pepper noise to the input tensor - randomly setting bits to 1 or 0.\n",
    "    Parameters\n",
    "    ----------\n",
    "    input : tensor\n",
    "        The tensor to apply salt and pepper noise to.\n",
    "    noise_level : float\n",
    "        The amount of salt and pepper noise to add.\n",
    "    Returns\n",
    "    -------\n",
    "    tensor\n",
    "        Tensor with salt and pepper noise applied.\n",
    "    \"\"\"\n",
    "    # salt and pepper noise\n",
    "    a = np.random.binomial(size=input.shape, n=1, p=(1 - noise_level)) # size is output shape, n is number of digits in output\n",
    "    b = np.random.binomial(size=input.shape, n=1, p=0.5)\n",
    "    # a and b are either 1 or 0, hence salt and pepper (either white or black)\n",
    "    c = (a==0) * b # if a is equal to 0, then c = b, otherwise c = 0\n",
    "    return input * a + c\n",
    "\n",
    "\n",
    "#data preparation, reshaped into a row vector instead of 28 by 28 matrix of pixels\n",
    "#adding noise using salt_and_pepper function\n",
    "flattened_x_train = x_train.reshape(-1,784)\n",
    "flattened_x_train_seasoned = salt_and_pepper(flattened_x_train, noise_level=0.4)\n",
    "\n",
    "flattened_x_test = x_test.reshape(-1,784)\n",
    "flattened_x_test_seasoned = salt_and_pepper(flattened_x_test, noise_level=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 96  \n",
    "\n",
    "input_image = keras.Input(shape=(784,))\n",
    "encoded = Dense(128, activation='relu')(input_image) # using the layer input_image as input to this layer of 128 neurons\n",
    "encoded = Dense(latent_dim, activation='relu')(encoded) # using previous encoded definition as input to this layer of 96 neurons\n",
    "decoded = Dense(128, activation='relu')(encoded) # using encoded layer (784, 128, 96) as input to this layer of 128 neurons\n",
    "decoded = Dense(784, activation='sigmoid')(decoded) # using previous decoded layer (784, 128, 96, 128) as input to this layer of 784 neurons\n",
    "\n",
    "autoencoder = keras.Model(input_image, decoded) # using all layers i.e. 784, 128, 96, 128, 784\n",
    "encoder_only = keras.Model(input_image, encoded) # using only encoded layers after input layer i.e. 784, 128, 96\n",
    "\n",
    "encoded_input = keras.Input(shape=(latent_dim,))\n",
    "decoder_layer = Sequential(autoencoder.layers[-2:]) # only decoded layers, no input layer i.e. 128, 784\n",
    "\n",
    "\n",
    "decoder = keras.Model(encoded_input, decoder_layer(encoded_input)) # input is encoded input followed by all decoded inputs i.e. 96, 128, 784 \n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_info_AE = autoencoder.fit(flattened_x_train_seasoned, flattened_x_train,\n",
    "                epochs=32,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                validation_data=(flattened_x_test_seasoned, flattened_x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The role of the loss function**\n",
    "\n",
    "The loss function describes the amount of information lost between the compressed and decompressed representations of the data examples and the decompressed representation, i.e what is lossed in the process of compressing and decompressing. Another way to view it could be to consider it as a function that calculates the reconstruction loss. The loss funciton used here is the same as the loss function in question 2 except we are using binary classification (i.e. there are only 2 classes so M = 2). This means that the loss function can be simplified to:\n",
    "\n",
    "  $L = \\dfrac{1}{N}\\sum_{o=1}^N{-(y_o\\cdot\\log{p_o}+(1-y_o)\\cdot\\log{(1-p_o)})} $\n",
    "  \n",
    "where $y_o$ is the expected output for that observation/data-point and $p_o$ is the probability that that data-point belongs to class *A* ($1-p_o$ is the probability that it is class *B*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Increasing levels of noise added to the test set using the salt_and_pepper()-function (0 to 1)**\n",
    "\n",
    "**A few examples are shown, with the original, \"seasoned\" (noisy), versions and denoised versions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the compressed images\n",
    "noise_levels = np.linspace(0, 1, 5)\n",
    "nr_images = 4\n",
    "random_sample = np.random.randint(x_test.shape[0], size=nr_images)\n",
    "for n in noise_levels:\n",
    "    # Applying noise to the input\n",
    "    noisy_images = salt_and_pepper(flattened_x_test, n)\n",
    "    # Decoded noisy images\n",
    "    decoded_images = autoencoder.predict(noisy_images)\n",
    "    print('Noise-level:', n)\n",
    "\n",
    "    plt.figure(figsize = (8, 5))\n",
    "    for i, j in enumerate(random_sample):\n",
    "        # Plot original image\n",
    "        ax = plt.subplot(3, nr_images, i + 1)\n",
    "        plt.imshow(flattened_x_test[j].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Plot noisy image\n",
    "        ax = plt.subplot(3, nr_images, nr_images + i + 1)\n",
    "        plt.imshow(noisy_images[j].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        # Plot denoised image\n",
    "        ax = plt.subplot(3, nr_images, 2*nr_images + i + 1)\n",
    "        plt.imshow(decoded_images[j].reshape(28, 28))\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, at noise-level 0.75, the decoded images starts to become indistinguishable. Although, numbers with a more unique shape, like 0, is still somewhat recognizeable. At noise-level 0.5 and below, the decoded images are clearly distinguishable, with exception to the 8 at noise-level 0.5. At noise-level 1 the images are indistinguishable, prehaps one could argue that some digit at a specific iteration is distinguishable, but that is probably with the preconcieved notion that it should be an certain digit. (Since the digits are sampled at random, particular digits mentioned in the text may not appear at the last execution (visualization) of the code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing if denoising improves the classification for the best performing model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure if this step below is necessary\n",
    "# model = keras.models.load_model()\n",
    "\n",
    "# Best model is the CNN from question 3\n",
    "best_model = conv_model\n",
    "prediction_scores_noisy = []\n",
    "prediction_scores_decoded = []\n",
    "\n",
    "for n in noise_levels:\n",
    "    # Add noise to images\n",
    "    noisy_images2 = salt_and_pepper(flattened_x_test, n)\n",
    "    # Decode noisy images\n",
    "    decoded_images2 = autoencoder.predict(noisy_images2)\n",
    "\n",
    "    # Seasoned dataset for the model (noisy images)\n",
    "    ni_reshaped = noisy_images2.reshape(len(noisy_images2), 28, 28, 1)\n",
    "    # Decoded dataset for the model\n",
    "    di_reshaped = decoded_images2.reshape(len(decoded_images2), 28, 28, 1)\n",
    "\n",
    "    # Model-prediction of the noisy images\n",
    "    print(f'Test accuracy of noisy digits at noise-level: {n}')\n",
    "    ni_info, ni_score = fit_and_eval(conv_model, x_train, y_train, 1, ni_reshaped, y_test, show_prog=1)\n",
    "    prediction_scores_noisy.append((ni_info, ni_score))\n",
    "    # Model-prediction of the decoded images\n",
    "    print(f'Test accuracy of decoded digits at noise-level: {n}')\n",
    "    di_info, di_score = fit_and_eval(conv_model, x_train, y_train, 1, di_reshaped, y_test, show_prog=1)\n",
    "    print()\n",
    "    prediction_scores_decoded.append((di_info, di_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the the results attained above as a function of noise-level for the seasoned and denoised datasets. \n",
    "n_scores = [s[1] for _, s in prediction_scores_noisy]\n",
    "d_scores = [s[1] for _, s in prediction_scores_decoded]\n",
    "#plt.figure(figsize=(10, 5))\n",
    "plt.scatter(noise_levels, n_scores, c='red', label='seasoned data')\n",
    "plt.plot(noise_levels, n_scores, c='red')\n",
    "plt.scatter(noise_levels, d_scores, c='green', label='denoised data')\n",
    "plt.plot(noise_levels, d_scores, c='green')\n",
    "plt.title('Accuracy of digit prediction with autoencoded data with the best preforming model')\n",
    "plt.xlabel('Noise-level')\n",
    "plt.ylabel('Prediction score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result presented in the graph has several interesting properties. When noise-level = 0, the model performs better on the seasoned data than the denoised data. This may initially seem surprising, but when inspecting closer it has an obvious answer. The seasoned data isn't actually seasoned at noise-level: 0, so the model performs as well as it can (at one epoch). The denoised data however, is processed and the autoencoder doesn't always succeed in recreating the data (digits) from the laten space, leading to some data being more difficult to predict than with the unmodified data. At all other noise-levels however, except for noise-level 1, the model performs significantly better on the denoised data, i.e. the recreated data, than the seasoned data. My assumption from the result before seems to be correct when interpeting the results; after noise-level: 0.5 the images from the auto-encoder starts to become indistinguishable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How the decoder part of the denoising auto-encoder can be used to generate synthetic \"hand-written\" digits**\n",
    "\n",
    "In order to create unique \"hand-written\" digits, one can use the auto-encoder (or just the encoder) in conjunction with the decoder. One can use the training data from beforehand and approach the problem where we use the autoencoder to create new, never before seen by the model, digits. Then use the image of the handwritten digit as input to the autoencoder and through the lossy compression creates a new handwritten digit, similar to the input. If one wants to alter the handwritten digit even more a certain amount of noise can be introduced as was done in the parts above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_decoded = autoencoder.predict(flattened_x_test_seasoned)\n",
    "\n",
    "imgs = np.concatenate([flattened_x_test_seasoned[:8], x_decoded[:8]])\n",
    "imgs = imgs.reshape((4, 4, 28, 28))\n",
    "imgs = np.vstack([np.hstack(i) for i in imgs])\n",
    "plt.figure()\n",
    "plt.axis('off')\n",
    "plt.title('Input: 1st 2 rows, Decoded: last 2 rows')\n",
    "plt.imshow(imgs, interpolation='none', cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "35a3ce59481b727575d263ea87fd9459b6c954f6b4fc9d2979322bbb4e156634"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
